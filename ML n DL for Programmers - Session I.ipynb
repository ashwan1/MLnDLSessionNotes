{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML n DL for Programmers\n",
    "-------------------------------\n",
    "### Session I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Intelligence\n",
    "* Inducing human like intelligent traits in machines.\n",
    "* Bit controversial.\n",
    "* Dozens of different definitions by different psycologist, scientists and philosophers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AGI (Artificial general intelligence)\n",
    "* Mostly like human. \n",
    "##### Examples:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### I DON'T KNOW\n",
    "![idontknow.jpg](images/idontknow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ANI (Artificial narrow intelligence)\n",
    "* Good in mostly one task. \n",
    "##### Example: AlphaGo\n",
    "![AlphaGo](images/alphago.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning\n",
    "* Algorithms that help machine learn by itself just by giving some examples.\n",
    "* Classic machine learning algorithms are:\n",
    "    - Support Vector trees\n",
    "    - Random forest\n",
    "    - K mean clustering\n",
    "    - And many more...\n",
    "* Quite outdated, but still good, if you have less data and want to set baseline for your performance.\n",
    "* More modern approaches are:\n",
    "    - Deep learning (Neural nets and their variants).\n",
    "* These outperform any other **classic** approaches given enough data and computing power.\n",
    "![DL Vs ML](images/dlvsml.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So here's a relationship\n",
    "![Relationship b/w AI, ML and DL](images/aimldl.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical ML vs DL\n",
    "\n",
    "| Classical ML | Deep learning |\n",
    "|--------------|---------------|\n",
    "|Requires Feature engineering| Extracts features by itself.|\n",
    "|![ML vs DL](images/machine-learning-vs-deep-learning.png)|\n",
    "|Requires comparitively less data and computations power| Some requires months of training on 100s of GPUs|\n",
    "|You can have some reasoning and insight on what algorithm is doing, sometimes its predictive too. | Its almost a black box.(Specially if you don't understand **math** behind it).|\n",
    "|![DL joke](images/dljoke.png)|\n",
    "|We will talk less about these| And more about these|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Teaching machines what to do...\n",
    "* We provide rules like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def is_even(number):\n",
    "    if number % 2 == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_even(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![functions](images/function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Problem:** not always possible (or atleast very tough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Write a program to recognize apples:\n",
    "![apple](images/apple-red.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![apple_green](images/apple-green.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![apple_white](images/apple-white.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![apple_rotated](images/apple-rotated.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learns what to do...\n",
    "![dl_iseven](images/dlmodel_iseven.png)\n",
    "\n",
    "**Notes**\n",
    "* We will be using python.\n",
    "* Some of the important python libraries for ML/DL:\n",
    "    - Pandas: To load and manipulate data.\n",
    "    - Matplotlib: To plot graphs over data.\n",
    "    - numpy: To do linear algebra.\n",
    "    - scikit-learn: Has many classical machine learning models implemented.\n",
    "    - Tensorflow: To do virtually everything in DL(mostly)\n",
    "    - Keras: Provides simple API as tensorflow backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ways machines can learn:\n",
    "\n",
    "* **Supervised learning:** We give both data and their corresponding labels explicitly. Eg. lots of cat images and telling that it is a cat.\n",
    "* **Unsupervised learning:** We give just raw data and algorithm finds pattern in it, without explicitly giving labels.\n",
    "* **Semi supervised learning:** When above two are mixed matched.\n",
    "* **Transfer learning:** When features learnt by one model is used by another. This is like calling  existing function for reuse (while sort of).\n",
    "* **Reinforcement learning:** Machine learns by doing itself and correcting itself at every mistake.\n",
    "* **Neural architecture search:** Search best suited network architecture via various ways.\n",
    "\n",
    "There are few more but they can be thought of as derivative of above learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of problems\n",
    "* Classification\n",
    "  - Whether it is cat or dog, truck or car, flower, fish, fruit, tree or .... you got the point\n",
    "* Regression\n",
    "  - Predicting some continous values. Eg. what will be tempreture tomorrow, price of house given various parameters.\n",
    "* Generative\n",
    "  - Generate image of something, generate new data, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### And playing games..\n",
    "![dota](images/dota2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basics of ML Math\n",
    "\n",
    "![math math every where meme](images/math-math-is.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![math required for machine learning image](images/mathinml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's begin\n",
    "\n",
    "* Scalars: Just a number.\n",
    "* Vectors: A vector is an array of numbers. \n",
    "\n",
    "     ![scalar and vector image](images/vectors.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Matrices: 2-D arrays\n",
    "\n",
    "  ![matrix](images/matrix-dimension1.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  \n",
    "* Tensors: >3 dimentional arrays\n",
    "\n",
    "  ![tesors](images/tensors.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now the learning part:\n",
    "\n",
    "![linear regression](images/weights.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cost functions\n",
    "* Most machine learning models are optimization problem. \n",
    "* We try to minimize error using some Loss function. Also called cost function.\n",
    "    - It simply is a measure of how wrong the model is in terms of its ability to estimate the relationship between input and output. Most simple lost function can be:\n",
    "        - Predicted value - Actual value - BUT YA! It will not work\n",
    "    - So here is a famous one:\n",
    "        - Mean-Squared Error cost function\n",
    "        ![MSE](images/meansqerror.png)\n",
    "\n",
    "**Note:** Using calculus, we know that the slope of a function is the derivative of the function with respect to a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "* The whole point of GD is to minimize the cost function.\n",
    "* **Gradient** is derivative of cost function with respect to weights. This turns out to be:\n",
    "    - ![gd](images/gd.png)\n",
    "  and gives direction in which we need to move. But how much should we move?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Enter your first Hyper-parameter - Learning Rate\n",
    "* A hyper-parameter is a value required by your model which we really have very little idea about.\n",
    "* Learning rate tells us how much to move in given direction, provided by GD.\n",
    "* So tweek this carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![gdgraph](images/gdgraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If that diagram is not enough ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### May be visualization can help\n",
    "![gdgif](images/gdgif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# And now _The Neural Network_\n",
    "\n",
    "![NN](images/neural-network_3.gif)\n",
    "\n",
    "##### Biologically-inspired method of building computer programs that are able to learn and independently find connections in data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single Neuron\n",
    "\n",
    "![single neuron](images/single_neuron.png)\n",
    "\n",
    "where $W^T.X = w_1x_1 + w_2x_2 + ... + w_3x_3$\n",
    "\n",
    "So basically it does:\n",
    "    - Input times Weight, add a Bias, Activate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single Layer\n",
    "![single layer](images/single_layer.png)\n",
    "\n",
    "Equation for a layer can be written in general as:\n",
    "![oneline](images/layeractivationoneline.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or if we elaborate a bit:\n",
    "![multi line eq](images/layeractivationmultiline.gif)\n",
    "\n",
    "* So we can do a simple for-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **So effective way of doint this is _matrix muliplication_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation functions\n",
    "\n",
    "* Neural nets can be called \n",
    "> **Universal Function Approximator**\n",
    "* Without activation function, they are just a linear function.\n",
    "* There are many activation function to choose from. Eg: Sigmoid, ReLU, tanh, etc.\n",
    "\n",
    "We are not always sure, which one to pick, so this is another......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Hyper-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common Activation Functions used:\n",
    "![activationfunc](images/activation_functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "* Fancy variant of gradient descent used in NN.\n",
    "* The parameters (weights and biases) of the neural network are adjusted according to the following formulae:<br>\n",
    "$W^{[l]} = W^{[l]} - \\alpha dW$ <br>\n",
    "$b^{[l]} = b^{[l]} - \\alpha db$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And also remember these:\n",
    "![bpeqmultiline](images/bpequaltionsmultiline.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![bp](images/backprogationimage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1> Theory wrapped up</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2> Be prepared for hands on classes ...</h2></center><br><br>\n",
    "<center><img src=\"images/beprepared.jpg\" /></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
